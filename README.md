# ğŸš´â€â™‚ï¸ Citibike NYC - Analyse Big Data avec Apache Spark

<div align="center">


**Analyse distribuÃ©e de 13.8 millions de trajets vÃ©lo avec Apache Spark**

*Projet acadÃ©mique INGC 225 - Plateformes Big Data | ESMT-Dakar 2025*

---

</div>

## ğŸ“– Ã€ Propos du Projet

Ce projet explore l'Ã©cosystÃ¨me **Apache Spark** Ã  travers l'analyse de donnÃ©es rÃ©elles du service de vÃ©los en libre-service **Citibike NYC**. L'objectif est de maÃ®triser le traitement distribuÃ© de donnÃ©es massives en rÃ©pondant Ã  7 questions analytiques sur **13,8 millions de trajets** effectuÃ©s en 2016.

### ğŸ¯ Objectifs PÃ©dagogiques

- **MaÃ®triser Apache Spark** : Configuration, SparkSession, DataFrames distribuÃ©s
- **Traiter des donnÃ©es massives** : Lecture, nettoyage, transformation de millions de lignes
- **Effectuer des agrÃ©gations complexes** : GroupBy, join, window functions
- **Visualiser des insights** : Graphiques avec Matplotlib Ã  partir de rÃ©sultats Spark
- **Optimiser les performances** : Cache, partitionnement, adaptive query execution

### ğŸ“Š DonnÃ©es AnalysÃ©es

| MÃ©trique | Valeur |
|----------|--------|
| **PÃ©riode** | AnnÃ©e 2016 complÃ¨te |
| **Trajets totaux** | 13,845,655 |
| **Fichiers CSV** | 12 (un par mois) |
| **Volume** | ~2-3 GB |
| **Stations** | 700+ dans NYC |

**Source** : [Citibike System Data](https://citibikenyc.com/system-data)

---

## ğŸ”¬ MÃ©thodologie

### Pipeline de Traitement

```mermaid
graph LR
    A[ğŸ“ 12 CSV Files] --> B[ğŸ”„ Spark Read]
    B --> C[ğŸ§¹ Cleaning]
    C --> D[ğŸ”§ Transformation]
    D --> E[ğŸ“Š Aggregation]
    E --> F[ğŸ“ˆ Visualization]
    
    style A fill:#e1f5ff
    style F fill:#e1ffe1
```

### DÃ©fis Techniques RelevÃ©s

1. **ğŸ”§ Formats de dates hÃ©tÃ©rogÃ¨nes** : Les fichiers CSV contiennent 4 formats diffÃ©rents de timestamps
   - **Solution** : Fonction `try_parse_multiple_formats()` avec `coalesce()` pour gÃ©rer tous les formats

2. **ğŸ’¾ Volume de donnÃ©es** : 13.8M lignes nÃ©cessitent une gestion mÃ©moire optimale
   - **Solution** : Utilisation de `cache()` et configuration appropriÃ©e de la mÃ©moire Spark

3. **ğŸ¨ Visualisation de rÃ©sultats distribuÃ©s** : Passer de Spark Ã  Matplotlib efficacement
   - **Solution** : Conversion ciblÃ©e avec `.toPandas()` uniquement sur les rÃ©sultats agrÃ©gÃ©s

---

## ğŸ““ Contenu du Notebook

Le notebook rÃ©pond aux **7 questions analytiques** suivantes :

### ğŸ”¢ Question 1 : Chargement des DonnÃ©es
- Lecture automatique de tous les CSV du rÃ©pertoire avec `os.walk()`
- CrÃ©ation du DataFrame distribuÃ© `tripdataDF`
- **RÃ©sultat** : 13,845,655 trajets chargÃ©s avec succÃ¨s

### ğŸ§¹ Question 2 : PrÃ©traitement et Standardisation
- **Innovation** : Fonction multi-format pour parser 4 types de timestamps diffÃ©rents
- Renommage des colonnes (snake_case pour la cohÃ©rence)
- Casting des types appropriÃ©s (int, double, timestamp)
- Filtrage des donnÃ©es invalides (durÃ©e > 0, dates non nulles)
- **RÃ©sultat** : Dataset propre et structurÃ©, prÃªt pour l'analyse

### ğŸ“… Question 3 : Trajets par Mois
- Extraction du mois depuis `starttime` avec `month()`
- AgrÃ©gation avec `groupBy().count()`
- **Visualisation** : Diagramme en bÃ¢tons (couleur bleu ciel)
- **Insight** : Forte saisonnalitÃ© avec pic en septembre (1.65M trajets) et creux en janvier (509K)

### ğŸ“† Question 4 : Tendance sur 365 Jours
- Extraction du jour de l'annÃ©e avec `dayofyear()`
- **Visualisation** : Courbe temporelle orange
- **Insight** : Pattern cyclique avec augmentation progressive vers l'Ã©tÃ©

### ğŸš‰ Question 5 : Utilisateurs par Station et Mois
- Utilisation de `countDistinct("bikeid")` pour compter les utilisateurs uniques
- Groupement par station de dÃ©part et mois
- **RÃ©sultat** : Matrice station Ã— mois Ã— nombre d'utilisateurs

### ğŸ† Question 6 : Taux de FrÃ©quentation des Stations
- Classement des stations par nombre de trajets
- **Visualisation** : Top 20 en barres horizontales vertes
- **Insight** : Concentration du trafic dans Manhattan (85% des trajets)

### â±ï¸ Question 7 : Statistiques de DurÃ©e
- Calcul de la durÃ©e moyenne, minimale et maximale par vÃ©lo
- **RÃ©sultat** : DurÃ©e moyenne de ~14 minutes, rÃ©vÃ©lant un usage principalement pour les courts trajets

---

## ğŸš€ Installation Rapide

### PrÃ©requis

- Python 3.8+
- Java JDK 8 ou 11 (requis pour Spark)
- Jupyter Notebook

### Installation en 3 Ã‰tapes

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/citibike-spark-analysis.git
cd citibike-spark-analysis

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. Lancer le notebook
jupyter notebook CitiBike_Analysis.ipynb
```

### DÃ©pendances (`requirements.txt`)

```txt
pyspark>=3.3.0
pandas>=1.5.0
numpy>=1.23.0
matplotlib>=3.6.0
seaborn>=0.12.0
jupyter>=1.0.0
```

---

## ğŸ“¥ TÃ©lÃ©chargement des DonnÃ©es

### Option 1 : TÃ©lÃ©chargement Manuel
Visitez [Citibike System Data](https://citibikenyc.com/system-data) et tÃ©lÃ©chargez les 12 fichiers CSV de 2016.

### Option 2 : Script Automatique
```bash
mkdir data && cd data
for month in {01..12}; do
    wget https://s3.amazonaws.com/tripdata/2016${month}-citibike-tripdata.zip
    unzip 2016${month}-citibike-tripdata.zip
    rm *.zip
done
```

---

## ğŸ’¡ Insights ClÃ©s

### ğŸŒ¡ï¸ SaisonnalitÃ© MarquÃ©e
- **Ã‰tÃ© (Juin-Sept)** : 70% du trafic annuel
- **Hiver (Dec-Fev)** : Seulement 13% du trafic
- **Ratio Ã©tÃ©/hiver** : x3 de diffÃ©rence

### ğŸ“ Concentration GÃ©ographique
- **Top 20 stations** : 15% du trafic total
- **Manhattan** : 85% des trajets
- **Zones touristiques** : Forte affluence toute l'annÃ©e

### â° Patterns d'Usage
- **DurÃ©e moyenne** : 14 minutes
- **Type d'utilisateur** : Majoritairement des abonnÃ©s (Subscribers)
- **Usage principal** : Trajet domicile-travail (commuting)

---

## ğŸ› ï¸ Technologies UtilisÃ©es

<div align="center">

| CatÃ©gorie | Stack |
|-----------|-------|
| **Big Data Processing** | Apache Spark 3.x, PySpark |
| **Langages** | Python 3.8+, Spark SQL |
| **Data Manipulation** | Pandas, NumPy |
| **Visualisation** | Matplotlib, Seaborn |
| **Environnement** | Jupyter Notebook |

</div>


---

## ğŸ“ CompÃ©tences Acquises

âœ… Configuration et utilisation d'Apache Spark en local  
âœ… Manipulation de DataFrames distribuÃ©s avec PySpark  
âœ… Gestion de formats de donnÃ©es hÃ©tÃ©rogÃ¨nes  
âœ… Optimisation de requÃªtes sur big data (cache, partitionnement)  
âœ… AgrÃ©gations complexes et fonctions de fenÃªtre  
âœ… Visualisation de rÃ©sultats analytiques  
âœ… Debugging et rÃ©solution de problÃ¨mes techniques  

---

## ğŸ› RÃ©solution de ProblÃ¨mes

### Erreur : `Java not found`
```bash
# Installer Java JDK 8 ou 11
# Configurer JAVA_HOME dans les variables d'environnement
```

### Erreur : `Out of Memory`
```python
# Augmenter la mÃ©moire Spark dans le notebook
spark = SparkSession.builder \
    .config("spark.driver.memory", "8g") \
    .getOrCreate()
```

### Erreur : Formats de dates
```python
# Le notebook inclut dÃ©jÃ  la solution avec try_parse_multiple_formats()
# Cette fonction gÃ¨re automatiquement les 4 formats prÃ©sents dans les donnÃ©es
```

---

## ğŸ“š Ressources

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Citibike System Data](https://citibikenyc.com/system-data)
- [Tutoriel Installation Spark Windows](https://nonlineardata.com/install-spark-pyspark-on-windows)

---

## ğŸ¤ Contribution

Les contributions sont bienvenues ! N'hÃ©sitez pas Ã  :
- ğŸ› Signaler des bugs
- ğŸ’¡ Proposer des amÃ©liorations
- ğŸ“Š Ajouter de nouvelles analyses
- ğŸ“ AmÃ©liorer la documentation

---


<div align="center">

## ğŸ“„ Licence

Projet acadÃ©mique dÃ©veloppÃ© dans le cadre du cours ** Plateformes Big Data**  

---

â­ **Si ce projet vous aide, donnez-lui une Ã©toile !**

*MaÃ®trisons ensemble le Big Data avec Apache Spark* ğŸš€

</div>